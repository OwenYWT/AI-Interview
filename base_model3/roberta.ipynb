{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "92e41810269ca32d"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "from dataset import InterviewDataset\n",
    "from huggingface_hub import login\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T20:14:36.007016Z",
     "start_time": "2024-11-26T20:14:36.001412Z"
    }
   },
   "id": "22a632c3b83f698"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "\n",
    "access_token = \"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\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") ## use GPU otherwise use CPU\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\", token=access_token)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\", token=access_token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T20:14:39.821512Z",
     "start_time": "2024-11-26T20:14:36.900079Z"
    }
   },
   "id": "c4a756bce10b6a83"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c85697c36d29a8a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T20:14:43.668798Z",
     "start_time": "2024-11-26T20:14:43.661398Z"
    }
   },
   "outputs": [],
   "source": [
    "config_path = \"config.yaml\"\n",
    "with open(config_path, \"r\") as file:\n",
    "\tconfig = yaml.safe_load(file)\n",
    "\n",
    "with open(config[\"data\"][\"train_data_path\"], \"r\") as f:\n",
    "\ttrain_data = json.load(f)\n",
    "with open(config[\"data\"][\"val_data_path\"], \"r\") as f:\n",
    "\tval_data = json.load(f)\n",
    "    \n",
    "train_dataset = InterviewDataset(train_data, tokenizer)\n",
    "val_dataset = InterviewDataset(val_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "\n",
    "access_token = \"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\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T20:14:51.654825Z",
     "start_time": "2024-11-26T20:14:51.647688Z"
    }
   },
   "id": "a9fedf281d3c6ce7"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "login(access_token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T20:10:33.055089Z",
     "start_time": "2024-11-26T20:10:32.705020Z"
    }
   },
   "id": "ba1e34bc90a038e7"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2474e9b8deae7ca",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T20:14:53.935760Z",
     "start_time": "2024-11-26T20:14:53.847306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 46893,   254,  ...,   254,    35,     2],\n",
      "        [    0, 46893,   254,  ..., 46893,  1942,     2],\n",
      "        [    0, 46893,   254,  ..., 25163,  2012,     2],\n",
      "        ...,\n",
      "        [    0, 46893,   254,  ...,  3050,    11,     2],\n",
      "        [    0, 46893,   254,  ...,   127,  1041,     2],\n",
      "        [    0, 46893,   254,  ...,    75,  1437,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[3.8880, 3.4144, 3.7052],\n",
      "        [3.6083, 3.2956, 4.7075],\n",
      "        [5.6093, 5.0431, 5.5066],\n",
      "        [6.4524, 6.4623, 6.2842],\n",
      "        [5.6609, 5.3001, 4.4353],\n",
      "        [5.6499, 5.4756, 4.9291],\n",
      "        [4.8644, 4.9387, 4.7271],\n",
      "        [3.9970, 3.7752, 3.8984]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_params = config[\"training\"]\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=training_params[\"learning_rate\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=training_params[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=training_params[\"batch_size\"])\n",
    "\n",
    "for batch in train_loader:\n",
    "\tprint(batch)\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b258544acf16c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-26T19:09:57.523239Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c131f4b34cb9f7e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T20:14:58.067161Z",
     "start_time": "2024-11-26T20:14:58.044697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaForMaskedLM(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (lm_head): RobertaLMHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n  )\n)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84b9b6460edfe32b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T20:17:23.699744Z",
     "start_time": "2024-11-26T20:17:23.662620Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from huggingface_hub import HfFolder\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\toutput_dir=\"./results\",\n",
    "\tnum_train_epochs=5,\n",
    "\tper_device_train_batch_size=8,\n",
    "\tper_device_eval_batch_size=8,\n",
    "\tevaluation_strategy=\"epoch\",\n",
    "\tlogging_strategy=\"steps\",\n",
    "\tlogging_steps=10,\n",
    "\tlearning_rate=5e-5,\n",
    "\tweight_decay=0.01,\n",
    "\twarmup_steps=500,\n",
    "\tsave_strategy=\"epoch\",\n",
    "\tload_best_model_at_end=True,\n",
    "\tsave_total_limit=2,\n",
    "\treport_to=\"tensorboard\",\n",
    "\tpush_to_hub=True,\n",
    "\thub_strategy=\"every_save\",\n",
    "\thub_token=HfFolder.get_token(),\n",
    "\tlogging_dir=\"./logs\",\n",
    "\tno_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer) \n",
    "\n",
    "trainer = Trainer(\n",
    "\tmodel=model,\n",
    "\targs=training_args,\n",
    "\ttrain_dataset=train_dataset,\n",
    "\teval_dataset=val_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T20:17:25.628424Z",
     "start_time": "2024-11-26T20:17:24.590516Z"
    }
   },
   "id": "d557792996507547"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53561204fa15cfac",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T20:16:00.656404Z",
     "start_time": "2024-11-26T20:15:34.399438Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (4096) to match target batch_size (24).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[49], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2114\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2111\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   2112\u001B[0m     \u001B[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001B[39;00m\n\u001B[1;32m   2113\u001B[0m     hf_hub_utils\u001B[38;5;241m.\u001B[39mdisable_progress_bars()\n\u001B[0;32m-> 2114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[1;32m   2115\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m   2116\u001B[0m         resume_from_checkpoint\u001B[38;5;241m=\u001B[39mresume_from_checkpoint,\n\u001B[1;32m   2117\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m   2118\u001B[0m         ignore_keys_for_eval\u001B[38;5;241m=\u001B[39mignore_keys_for_eval,\n\u001B[1;32m   2119\u001B[0m     )\n\u001B[1;32m   2120\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   2121\u001B[0m     hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2481\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2475\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2476\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   2477\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2478\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[1;32m   2479\u001B[0m )\n\u001B[1;32m   2480\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[0;32m-> 2481\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs, num_items_in_batch)\n\u001B[1;32m   2483\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2484\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2485\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2486\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2487\u001B[0m ):\n\u001B[1;32m   2488\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2489\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:3579\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m   3576\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   3578\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 3579\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss(model, inputs, num_items_in_batch\u001B[38;5;241m=\u001B[39mnum_items_in_batch)\n\u001B[1;32m   3581\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[1;32m   3582\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   3583\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3584\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   3585\u001B[0m ):\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:3633\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m   3631\u001B[0m         loss_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_items_in_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_items_in_batch\n\u001B[1;32m   3632\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mloss_kwargs}\n\u001B[0;32m-> 3633\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n\u001B[1;32m   3634\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   3635\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   3636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:1225\u001B[0m, in \u001B[0;36mRobertaForMaskedLM.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1223\u001B[0m     labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(prediction_scores\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   1224\u001B[0m     loss_fct \u001B[38;5;241m=\u001B[39m CrossEntropyLoss()\n\u001B[0;32m-> 1225\u001B[0m     masked_lm_loss \u001B[38;5;241m=\u001B[39m loss_fct(prediction_scores\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mvocab_size), labels\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m   1227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict:\n\u001B[1;32m   1228\u001B[0m     output \u001B[38;5;241m=\u001B[39m (prediction_scores,) \u001B[38;5;241m+\u001B[39m outputs[\u001B[38;5;241m2\u001B[39m:]\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:1293\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m   1292\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mcross_entropy(\n\u001B[1;32m   1294\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   1295\u001B[0m         target,\n\u001B[1;32m   1296\u001B[0m         weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight,\n\u001B[1;32m   1297\u001B[0m         ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mignore_index,\n\u001B[1;32m   1298\u001B[0m         reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduction,\n\u001B[1;32m   1299\u001B[0m         label_smoothing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_smoothing,\n\u001B[1;32m   1300\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3479\u001B[0m, in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   3477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3478\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 3479\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39mcross_entropy_loss(\n\u001B[1;32m   3480\u001B[0m     \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   3481\u001B[0m     target,\n\u001B[1;32m   3482\u001B[0m     weight,\n\u001B[1;32m   3483\u001B[0m     _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction),\n\u001B[1;32m   3484\u001B[0m     ignore_index,\n\u001B[1;32m   3485\u001B[0m     label_smoothing,\n\u001B[1;32m   3486\u001B[0m )\n",
      "\u001B[0;31mValueError\u001B[0m: Expected input batch_size (4096) to match target batch_size (24)."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "14403a20eefe936"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
