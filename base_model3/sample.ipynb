{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoConfig,\n",
    ")\n",
    "from huggingface_hub import HfFolder, login"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T00:01:19.505854Z",
     "start_time": "2024-11-27T00:01:19.488095Z"
    }
   },
   "id": "ce8ae2d2a0018023"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model_id = \"roberta-base\"\n",
    "dataset_id = \"ag_news\"\n",
    "access_token = \"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\"\n",
    "login(access_token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T00:01:21.992837Z",
     "start_time": "2024-11-27T00:01:21.055654Z"
    }
   },
   "id": "589b7f48542dbad"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Columns: Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 120000\n",
      "})\n",
      "Validation Dataset Columns: Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 3800\n",
      "})\n",
      "Test Dataset Columns: Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 3800\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "# Training and testing datasets\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset[\"test\"].shard(num_shards=2, index=0)\n",
    "\n",
    "# Validation dataset\n",
    "val_dataset = dataset['test'].shard(num_shards=2, index=1)\n",
    "\n",
    "\n",
    "print(\"Train Dataset Columns:\", train_dataset)\n",
    "print(\"Validation Dataset Columns:\", val_dataset)\n",
    "print(\"Test Dataset Columns:\", test_dataset)\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "# This function tokenizes the input text using the RoBERTa tokenizer. \n",
    "# It applies padding and truncation to ensure that all sequences have the same length (256 tokens).\n",
    "def tokenize(batch):\n",
    "\treturn tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T00:06:16.330859Z",
     "start_time": "2024-11-27T00:06:12.983702Z"
    }
   },
   "id": "8e0da1241a696dac"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Set dataset format\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T00:01:42.921865Z",
     "start_time": "2024-11-27T00:01:42.917861Z"
    }
   },
   "id": "ee19e502d2e3d771"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 4\n",
      "the labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "# We will need this to directly output the class names when using the pipeline without mapping the labels later.\n",
    "# Extract the number of classes and their names\n",
    "num_labels = dataset['train'].features['label'].num_classes\n",
    "class_names = dataset[\"train\"].features[\"label\"].names\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")\n",
    "\n",
    "# Create an id2label mapping\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "# Update the model's configuration with the id2label mapping\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "config.update({\"id2label\": id2label})\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-27T00:01:48.079473Z",
     "start_time": "2024-11-27T00:01:47.828047Z"
    }
   },
   "id": "56a6673c1c6ddf11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def restructure_dataset(file_path):\n",
    "\tdata = pd.read_json(file_path)\n",
    "\n",
    "\tstructured_data = pd.DataFrame([\n",
    "\t\t{\n",
    "\t\t\t\"Overall\": item.get(\"Overall\"),\n",
    "\t\t\t\"RecommendHiring\": item.get(\"RecommendHiring\"),\n",
    "\t\t\t\"StructuredAnswers\": item.get(\"StructuredAnswers\"),\n",
    "\t\t\t\"Transcript\": item.get(\"Transcript\")\n",
    "\t\t}\n",
    "\t\tfor key, item in data.items()\n",
    "\t])\n",
    "\n",
    "\treturn structured_data\n",
    "\n",
    "train_dataset = restructure_dataset(\"data/train_data.json\")\n",
    "val_dataset = restructure_dataset(\"data/val_data.json\")\n",
    "test_dataset = restructure_dataset(\"data/data.json\")\n",
    "\n",
    "print(\"Train Dataset Columns:\", train_dataset)\n",
    "print(\"Validation Dataset Columns:\", val_dataset.columns)\n",
    "print(\"Test Dataset Columns:\", test_dataset.columns)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69e5e8da4ad12c04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "model_id = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "def tokenize_and_format(dataset, text_column=\"Transcript\"):\n",
    "\tinput_ids = []\n",
    "\tattention_masks = []\n",
    "\tlabels = []\n",
    "\n",
    "\tfor idx in range(len(dataset)):\n",
    "\t\ttranscript = dataset.iloc[idx][text_column]\n",
    "\t\tlabel = torch.tensor([\n",
    "\t\t\tdataset.iloc[idx][\"Overall\"],\n",
    "\t\t\tdataset.iloc[idx][\"RecommendHiring\"],\n",
    "\t\t\tdataset.iloc[idx][\"StructuredAnswers\"],\n",
    "\t\t], dtype=torch.float)\n",
    "\n",
    "\t\tinputs = tokenizer(\n",
    "\t\t\ttranscript,\n",
    "\t\t\tmax_length=256,\n",
    "\t\t\tpadding=\"max_length\",\n",
    "\t\t\ttruncation=True,\n",
    "\t\t\treturn_tensors=\"pt\"\n",
    "\t\t)\n",
    "\n",
    "\t\tinput_ids.append(inputs[\"input_ids\"].squeeze(0))\n",
    "\t\tattention_masks.append(inputs[\"attention_mask\"].squeeze(0))\n",
    "\t\tlabels.append(label)\n",
    "\n",
    "\treturn {\n",
    "\t\t\"input_ids\": torch.stack(input_ids),\n",
    "\t\t\"attention_mask\": torch.stack(attention_masks),\n",
    "\t\t\"labels\": torch.stack(labels)\n",
    "\t}\n",
    "\n",
    "train_data = tokenize_and_format(train_dataset, text_column=\"Transcript\")\n",
    "val_data = tokenize_and_format(val_dataset, text_column=\"Transcript\")\n",
    "test_data = tokenize_and_format(test_dataset, text_column=\"Transcript\")\n",
    "\n",
    "print(f\"Train Dataset Size: {train_data['input_ids'].size(0)} samples\")\n",
    "print(f\"Validation Dataset Size: {val_data['input_ids'].size(0)} samples\")\n",
    "print(f\"Test Dataset Size: {test_data['input_ids'].size(0)} samples\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bf210bd70322f2c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\tdef __init__(self, data):\n",
    "\t\tself.input_ids = data[\"input_ids\"]\n",
    "\t\tself.attention_mask = data[\"attention_mask\"]\n",
    "\t\tself.labels = data[\"labels\"]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.input_ids)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn {\n",
    "\t\t\t\"input_ids\": self.input_ids[idx],\n",
    "\t\t\t\"attention_mask\": self.attention_mask[idx],\n",
    "\t\t\t\"labels\": self.labels[idx]\n",
    "\t\t}\n",
    "\n",
    "train_dataset = CustomDataset(train_data)\n",
    "val_dataset = CustomDataset(val_data)\n",
    "test_dataset = CustomDataset(test_data)\n",
    "\n",
    "\n",
    "print(f\"Train Dataset Size: {len(train_dataset)} samples\")\n",
    "print(f\"Validation Dataset Size: {len(val_dataset)} samples\")\n",
    "print(f\"Test Dataset Size: {len(test_dataset)} samples\")\n",
    "\n",
    "print(\"Sample Labels from Train Dataset:\")\n",
    "for i in range(5):\n",
    "\tsample = train_dataset[i]\n",
    "\tprint(f\"Sample {i} Label: {sample['labels']}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d9d8f368f4813a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "for batch in train_loader:\n",
    "\tprint(batch[\"input_ids\"].size(), batch[\"attention_mask\"].size(), batch[\"labels\"].size())\n",
    "\tbreak\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2de086b63a3a299c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "num_labels = 3\n",
    "class_names = [\"Overall\", \"RecommendHiring\", \"StructuredAnswers\"]\n",
    "\n",
    "# Print information about the labels\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")\n",
    "\n",
    "# Create an id2label mapping for interpretability (optional)\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "# Update the model's configuration with the id2label mapping\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "config.update({\"id2label\": id2label, \"num_labels\": num_labels})\n",
    "\n",
    "print(\"Updated model configuration:\", config)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc3736e8c50ac5d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.optim import AdamW\n",
    "# from tqdm import tqdm\n",
    "# from transformers import RobertaForSequenceClassification, AutoConfig\n",
    "# from torch.nn.functional import mse_loss\n",
    "# \n",
    "# class CustomTrainer:\n",
    "# \tdef __init__(self, model, train_loader, val_loader, optimizer, device, max_epochs):\n",
    "# \t\tself.model = model.to(device)\n",
    "# \t\tself.train_loader = train_loader\n",
    "# \t\tself.val_loader = val_loader\n",
    "# \t\tself.optimizer = optimizer\n",
    "# \t\tself.device = device\n",
    "# \t\tself.max_epochs = max_epochs\n",
    "# \n",
    "# \tdef train(self):\n",
    "# \t\tfor epoch in range(self.max_epochs):\n",
    "# \t\t\tself.model.train()\n",
    "# \t\t\ttrain_loss = 0.0\n",
    "# \t\t\tfor batch in tqdm(self.train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "# \t\t\t\t# Move inputs and labels to device\n",
    "# \t\t\t\tinput_ids = batch[\"input_ids\"].to(self.device)\n",
    "# \t\t\t\tattention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "# \t\t\t\tlabels = batch[\"labels\"].to(self.device)\n",
    "# \n",
    "# \t\t\t\t# Forward pass\n",
    "# \t\t\t\toutputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "# \t\t\t\tloss = outputs.loss\n",
    "# \n",
    "# \t\t\t\t# Backward pass\n",
    "# \t\t\t\tself.optimizer.zero_grad()\n",
    "# \t\t\t\tloss.backward()\n",
    "# \t\t\t\tself.optimizer.step()\n",
    "# \n",
    "# \t\t\t\ttrain_loss += loss.item()\n",
    "# \n",
    "# \t\t\tavg_train_loss = train_loss / len(self.train_loader)\n",
    "# \t\t\tprint(f\"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f}\")\n",
    "# \n",
    "# \t\t\t# Evaluate at the end of each epoch\n",
    "# \t\t\tself.evaluate()\n",
    "# \n",
    "# \tdef evaluate(self):\n",
    "# \t\tself.model.eval()\n",
    "# \t\tval_loss = 0.0\n",
    "# \t\twith torch.no_grad():\n",
    "# \t\t\tfor batch in tqdm(self.val_loader, desc=\"Validation\"):\n",
    "# \t\t\t\t# Move inputs and labels to device\n",
    "# \t\t\t\tinput_ids = batch[\"input_ids\"].to(self.device)\n",
    "# \t\t\t\tattention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "# \t\t\t\tlabels = batch[\"labels\"].to(self.device)\n",
    "# \n",
    "# \t\t\t\t# Forward pass\n",
    "# \t\t\t\toutputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "# \t\t\t\tval_loss += outputs.loss.item()\n",
    "# \n",
    "# \t\tavg_val_loss = val_loss / len(self.val_loader)\n",
    "# \t\tprint(f\"Validation Loss: {avg_val_loss:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c311abeb5571236d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from trainer import Trainer\n",
    "# Model\n",
    "config = AutoConfig.from_pretrained(model_id, num_labels=3)  # Assuming 3 labels (Overall, RecommendHiring, StructuredAnswers)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_id, config=config)\n",
    "\n",
    "# Dataloaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=0.00005, weight_decay=0.01)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "\tmodel=model,\n",
    "\ttrain_loader=train_loader,\n",
    "\tval_loader=val_loader,\n",
    "\toptimizer=optimizer,\n",
    "\tdevice=device,\n",
    "\tmax_epochs=5\n",
    ")\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d45d375ec90edd4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_true_scores = []\n",
    "all_predicted_scores = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\tfor batch in test_loader:\n",
    "\t\t# Move input data and labels to the device\n",
    "\t\tinputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "\t\tlabels = batch[\"labels\"].to(device)\n",
    "\n",
    "\t\t# Get model predictions\n",
    "\t\toutputs = model(**inputs)\n",
    "\n",
    "\t\t# Assuming the outputs are logits, convert them to probabilities\n",
    "\t\tlogits = outputs.logits\n",
    "\t\tprobabilities = torch.softmax(logits, dim=-1)  # Adjust based on task\n",
    "\n",
    "\t\t# For classification, take the argmax\n",
    "\t\tpredictions = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "\t\t# Collect true labels and predictions\n",
    "\t\tall_true_scores.extend(labels.cpu().numpy())\n",
    "\t\tall_predicted_scores.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Convert results to tensors\n",
    "all_true_scores_tensor = torch.tensor(all_true_scores)\n",
    "all_predicted_scores_tensor = torch.tensor(all_predicted_scores)\n",
    "\n",
    "# Print scores\n",
    "print(\"True Scores (Labels):\", all_true_scores_tensor)\n",
    "print(\"Predicted Scores:\", all_predicted_scores_tensor)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d11fd3304b16e37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
