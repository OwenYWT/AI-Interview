{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataset import InterviewDataset,HierarchicalInterviewDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yaml\n",
    "from transformers import AlbertTokenizer,Trainer,TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"config.yaml\"\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "with open(config[\"train\"][\"train_data_path\"], \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(config[\"train\"][\"val_data_path\"], \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "with open(config[\"train\"][\"test_data_path\"], \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "train_dataset = InterviewDataset(train_data, tokenizer)\n",
    "val_dataset = InterviewDataset(val_data, tokenizer)\n",
    "test_dataset = InterviewDataset(test_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"train\"][\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"train\"][\"batch_size\"], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HongzhenAlbertForRegression(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertSdpaAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (regressor): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from model import HongzhenAlbertForRegression\n",
    "\n",
    "model = HongzhenAlbertForRegression(\"albert-base-v2\", num_outputs=3)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0967,  0.4846,  0.3923],\n",
      "        [-0.2326,  0.2748,  0.4676],\n",
      "        [ 0.0290,  0.4234,  0.3318],\n",
      "        [ 0.0973,  0.3607,  0.4090],\n",
      "        [-0.0449,  0.3300,  0.3601],\n",
      "        [-0.0444,  0.3254,  0.4808],\n",
      "        [ 0.0092,  0.2525,  0.4913],\n",
      "        [-0.0368,  0.3562,  0.4880]], device='mps:0')\n",
      "tensor([[ 0.1112,  0.3248,  0.4090],\n",
      "        [-0.0814,  0.3070,  0.3374],\n",
      "        [-0.0473,  0.2497,  0.3739],\n",
      "        [-0.0700,  0.3076,  0.4103],\n",
      "        [ 0.0026,  0.3451,  0.2923],\n",
      "        [-0.0264,  0.3542,  0.4904],\n",
      "        [ 0.0422,  0.4384,  0.5120],\n",
      "        [-0.1210,  0.2908,  0.5706]], device='mps:0')\n",
      "tensor([[-0.0347,  0.4285,  0.5229],\n",
      "        [-0.0660,  0.3340,  0.4349],\n",
      "        [ 0.0794,  0.3655,  0.3892],\n",
      "        [-0.0702,  0.4349,  0.5191],\n",
      "        [-0.0394,  0.3432,  0.5054],\n",
      "        [ 0.0115,  0.4470,  0.3762],\n",
      "        [ 0.0476,  0.3815,  0.3595],\n",
      "        [ 0.0728,  0.4012,  0.3219]], device='mps:0')\n",
      "tensor([[-0.0794,  0.3223,  0.3005],\n",
      "        [ 0.0306,  0.3017,  0.5001],\n",
      "        [ 0.0276,  0.4247,  0.4057],\n",
      "        [-0.0417,  0.4360,  0.4835]], device='mps:0')\n",
      "Validation Loss Before Training: 21.895864963531494\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device) \n",
    "        \n",
    "        loss, _ = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        print(_)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "print(f\"Validation Loss Before Training: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hongzhenliang/miniconda3/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='840' max='840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [840/840 10:39, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.492300</td>\n",
       "      <td>1.576648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.594600</td>\n",
       "      <td>0.565109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.609600</td>\n",
       "      <td>0.974034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.455500</td>\n",
       "      <td>0.530158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.707900</td>\n",
       "      <td>0.553858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.527100</td>\n",
       "      <td>0.602852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.327500</td>\n",
       "      <td>0.615821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.443100</td>\n",
       "      <td>0.645244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>0.675914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>0.551686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>0.574537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.109400</td>\n",
       "      <td>0.564124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.676863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.294100</td>\n",
       "      <td>0.540315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.501450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>0.529154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>0.533865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>0.596492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.628614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.071300</td>\n",
       "      <td>0.546584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.080300</td>\n",
       "      <td>0.555980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.544984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>0.514175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.520089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.516891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.049100</td>\n",
       "      <td>0.521499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.510766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.511986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.521535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.521336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 639.98 seconds\n",
      "Average Latency: 0.2546 seconds per batch\n",
      "Throughput: 15.71 samples per second\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=config[\"train\"][\"max_epochs\"],\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "trainer.save_model(\"./saved_model\")\n",
    "\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "\n",
    "test_batch = next(iter(torch.utils.data.DataLoader(val_dataset, batch_size=4)))\n",
    "\n",
    "input_ids = test_batch[\"input_ids\"].to(device)\n",
    "attention_mask = test_batch[\"attention_mask\"].to(device)\n",
    "labels = test_batch[\"labels\"].to(device)\n",
    "\n",
    "start_inference = time.time()\n",
    "\n",
    "num_inference_steps = 100\n",
    "for _ in range(num_inference_steps):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "end_inference = time.time()\n",
    "\n",
    "total_inference_time = end_inference - start_inference\n",
    "latency = total_inference_time / num_inference_steps\n",
    "print(f\"Average Latency: {latency:.4f} seconds per batch\")\n",
    "\n",
    "throughput = (len(input_ids) * num_inference_steps) / total_inference_time\n",
    "print(f\"Throughput: {throughput:.2f} samples per second\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.4447, 5.4398, 5.1086],\n",
      "        [5.4919, 5.2047, 5.1524],\n",
      "        [4.9649, 4.6426, 4.5099],\n",
      "        [5.1574, 5.1464, 4.5311],\n",
      "        [4.7565, 4.5725, 4.3863],\n",
      "        [4.7968, 4.4531, 4.2102],\n",
      "        [5.0020, 4.8677, 4.4535],\n",
      "        [5.6236, 5.5630, 4.9796]], device='mps:0')\n",
      "tensor([[5.2971, 5.3734, 5.0644],\n",
      "        [5.2689, 5.0842, 5.1370],\n",
      "        [4.8759, 4.7342, 4.4613],\n",
      "        [4.7673, 4.4050, 4.5188],\n",
      "        [5.1244, 5.0401, 4.7546],\n",
      "        [5.0865, 4.8014, 4.6703],\n",
      "        [5.1822, 5.1073, 4.6776],\n",
      "        [5.1031, 4.7859, 4.7913]], device='mps:0')\n",
      "tensor([[4.7609, 4.5866, 4.7031],\n",
      "        [4.9065, 4.6431, 4.5570],\n",
      "        [5.5202, 5.2732, 5.3021],\n",
      "        [5.0789, 4.8670, 4.8670],\n",
      "        [5.2089, 5.0239, 4.6399],\n",
      "        [5.0796, 4.9605, 4.6894],\n",
      "        [5.4122, 5.2158, 5.1943],\n",
      "        [5.1661, 5.0434, 4.6983]], device='mps:0')\n",
      "tensor([[4.6604, 4.5488, 4.1597],\n",
      "        [5.0230, 4.8539, 4.8330],\n",
      "        [4.7692, 4.4952, 3.9408],\n",
      "        [4.5431, 4.2835, 4.4594]], device='mps:0')\n",
      "Validation Loss Before Training: 0.47450462728738785\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device) \n",
    "        \n",
    "        loss, _ = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        print(_)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "print(f\"Validation Loss Before Training: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.021505832672119 tensor([[5.4674, 5.3285, 5.6962]], device='mps:0')\n",
      "0.590577244758606 tensor([[5.3224, 5.1571, 5.6184]], device='mps:0')\n",
      "0.02622946910560131 tensor([[4.8893, 4.6117, 4.9413]], device='mps:0')\n",
      "Validation Loss Before Training: 0.8794375155121088\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device) \n",
    "        \n",
    "        loss, _ = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        print(loss.item(),_)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "print(f\"Validation Loss Before Training: {avg_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
