{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import yaml\n",
    "import pprint\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model import HierarchicalInterviewScorer\n",
    "from dataset import HierarchicalInterviewDataset\n",
    "from eval import calculate_feedback\n",
    "from trainer import Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from model_utils import tokenize_dialogue, predict_scores, format_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"config.yaml\"\n",
    "checkpoint_path = \"checkpoin1.pth\"\n",
    "test_data_path = \"data.json\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "device = torch.device(config[\"train\"][\"device\"])\n",
    "\n",
    "with open(test_data_path, \"r\") as f:\n",
    "    test_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved to data/train_data.json with 110 samples.\n",
      "Validation data saved to data/val_data.json with 28 samples.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_path = \"data.json\" \n",
    "with open(data_path, \"r\") as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "if not isinstance(full_data, dict):\n",
    "    raise ValueError(\"Expected 'full_data' to be a dictionary.\")\n",
    "\n",
    "keys = list(full_data.keys())\n",
    "train_keys, val_keys = train_test_split(keys, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = {key: full_data[key] for key in train_keys}\n",
    "val_data = {key: full_data[key] for key in val_keys}\n",
    "\n",
    "train_data_path = \"data/train_data.json\"\n",
    "val_data_path = \"data/val_data.json\"\n",
    "\n",
    "with open(train_data_path, \"w\") as f:\n",
    "    json.dump(train_data, f, indent=4)\n",
    "    \n",
    "with open(val_data_path, \"w\") as f:\n",
    "    json.dump(val_data, f, indent=4)\n",
    "\n",
    "print(f\"Training data saved to {train_data_path} with {len(train_data)} samples.\")\n",
    "print(f\"Validation data saved to {val_data_path} with {len(val_data)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"data/train_data.json\"\n",
    "val_data_path = \"data/val_data.json\"\n",
    "\n",
    "with open(train_data_path, \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(val_data_path, \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "train_dataset = HierarchicalInterviewDataset(train_data)\n",
    "val_dataset = HierarchicalInterviewDataset(val_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"train\"][\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"train\"][\"batch_size\"], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HierarchicalInterviewScorer().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"train\"][\"learning_rate\"])\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    max_epochs=config[\"train\"][\"max_epochs\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:15<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 5.828921624592373\n",
      "Validation Loss: 0.4427642151713371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:15<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.810506556715284\n",
      "Validation Loss: 0.8056420311331749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:15<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.7978470836366925\n",
      "Validation Loss: 0.6536044850945473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:15<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.7968785762786865\n",
      "Validation Loss: 0.509797215461731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:15<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.7355893445866448\n",
      "Validation Loss: 0.8296225219964981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:15<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.6988619012492043\n",
      "Validation Loss: 0.6281289607286453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:15<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.6435613696064267\n",
      "Validation Loss: 0.6464710086584091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:15<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.6761828341654369\n",
      "Validation Loss: 0.5492907911539078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:16<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.7211620488337108\n",
      "Validation Loss: 0.7410292029380798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:16<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.745751406465258\n",
      "Validation Loss: 0.6570994555950165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:16<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 0.8036160341330937\n",
      "Validation Loss: 0.6187163889408112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:17<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 0.6884336067097527\n",
      "Validation Loss: 0.5743231549859047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:17<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 0.7872324160167149\n",
      "Validation Loss: 1.030325673520565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:16<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 0.7126115124140467\n",
      "Validation Loss: 0.49297795444726944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:17<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 0.687884047627449\n",
      "Validation Loss: 0.8639805912971497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:16<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train Loss: 0.7557264694145748\n",
      "Validation Loss: 0.7659858465194702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:16<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Train Loss: 0.6286451433386121\n",
      "Validation Loss: 0.6017658561468124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:17<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train Loss: 0.6652419183935437\n",
      "Validation Loss: 0.47147973626852036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:17<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Train Loss: 0.7107865065336227\n",
      "Validation Loss: 0.7395778745412827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:17<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Train Loss: 0.6854832353336471\n",
      "Validation Loss: 0.7612397894263268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:17<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | Train Loss: 0.6210690992219108\n",
      "Validation Loss: 0.6595550999045372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:16<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | Train Loss: 0.7282033903258187\n",
      "Validation Loss: 0.586664155125618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:17<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | Train Loss: 0.6055919783455985\n",
      "Validation Loss: 0.8000767976045609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:17<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | Train Loss: 0.5805834255048207\n",
      "Validation Loss: 0.7724829390645027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:17<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | Train Loss: 0.6373833758490426\n",
      "Validation Loss: 0.7732015252113342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:17<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 | Train Loss: 0.5720645657607487\n",
      "Validation Loss: 0.672889344394207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:17<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 | Train Loss: 0.6040290700537818\n",
      "Validation Loss: 0.8964478000998497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:18<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 | Train Loss: 0.6023351294653756\n",
      "Validation Loss: 0.51471097022295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:17<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 | Train Loss: 0.5764913633465767\n",
      "Validation Loss: 1.1227686703205109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:19<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 | Train Loss: 0.5617868580988475\n",
      "Validation Loss: 0.5746353231370449\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HierarchicalInterviewDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"train\"][\"batch_size\"], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"checkpoint1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"checkpoint1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_true_scores = []\n",
    "all_predicted_scores = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for dialogue_turns, labels in test_loader:\n",
    "        dialogue_turns = {k: v.to(device) for k, v in dialogue_turns.items()}\n",
    "        labels = labels.to(device)\n",
    "        predictions = model(dialogue_turns)\n",
    "        all_true_scores.extend(labels.cpu().numpy())\n",
    "        all_predicted_scores.extend(predictions.cpu().numpy())\n",
    "\n",
    "all_true_scores_tensor = torch.tensor(all_true_scores)\n",
    "all_predicted_scores_tensor = torch.tensor(all_predicted_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feedback = calculate_feedback(all_predicted_scores_tensor, all_true_scores_tensor)\n",
    "\n",
    "# import pprint\n",
    "# pp = pprint.PrettyPrinter(indent=4)\n",
    "# for idx, candidate_feedback in enumerate(feedback):\n",
    "#     print(f\"Feedback for Candidate {idx + 1}:\")\n",
    "#     pp.pprint(candidate_feedback)\n",
    "#     print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(checkpoint_path, device=\"cpu\"):\n",
    "    from model import HierarchicalInterviewScorer\n",
    "\n",
    "    model = HierarchicalInterviewScorer(hidden_size=768, num_dialogue_layers=2, dropout=0.3)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/938010/ipykernel_1046282/584703163.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "model = load_trained_model(\"checkpoint1.pth\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I'm doing well, thank you!\",\n",
    "    \"What about yourself?\",\n",
    "    \"I'm good too. It's a sunny day.\"\n",
    "]\n",
    "\n",
    "tokenized_input = tokenize_dialogue(conversation, max_turns=20, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Scores: {'Overall Score': 4.872854709625244, 'Recommendation Score': 4.576309680938721, 'Structured Answers Score': 4.527109622955322}\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = {k: v.to(device) for k, v in tokenized_input.items()}\n",
    "\n",
    "predicted_scores = predict_scores(model, tokenized_input, device=device)\n",
    "\n",
    "formatted_scores = format_scores(predicted_scores)\n",
    "print(\"Predicted Scores:\", formatted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation samples: 28\n",
      "Example Sample: pp55\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "val_data_path = \"data/val_data.json\"\n",
    "with open(val_data_path, \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "print(f\"Number of validation samples: {len(val_data)}\")\n",
    "print(\"Example Sample:\", list(val_data.keys())[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Key: pp55\n",
      "Sample Dialogue: Interviewer: Ready?|Interviewee: Yeah  sure.|Interviewer: How are you doing today?|Interviewee: Ummm  I'm great. Thank you.|Interviewer: OK Tell me a bit about your background|Interviewee: OK well I am a math major  but I have a lot of background in kind of different areas. So freshman year I did some work  um  at the Department of Energy  um  working the CFO's office so I'm getting kind of a feel for the budget um also risk matrices. Then I kinda branched more towards CS aspects  so  I did a bit of coding here and there in most research aspects and also  um  for companies.|Interviewer: Great. And do you think you are a leader  and why?|Interviewee: Ummm  definitely. So I'm president of The Undergrad Society of Learner Math and this basically gives me a great opportunity to  um  kind of help underclassmen and  also  younger girls  so those in highschool  middle school and kind of get them to become more active in the math community um participate in more events  even networking things  so  uh  anything with companies but also research with professors. It's a great way to kind of help other people who haven't really seen those opportunities yet.|Interviewer: And do you consider yourself a team player? Can you give an example of when there was a conflict in your team and you resolved it?|Interviewee: Yeah  soum  last winter I was working in the Boston area at Autotegrity which is like a online marketing start up. um   I don't know how much familiarity you have with  um  coding like repository systems  but  basically if someone  um  uploads like their work to the server and it doesn't work basically everyone elses stuff doesn't work either. So there's like a little column button type thing in rebuild which is called the blame button so you could kind of blame people. So every once in a while it would occur where someone would screw up the code and that would make everyone's work stop instead of you know blaming the one person telling them to fix the code we would all kind of look at where it went wrong and how we could improve upon that further instead of just fixing one persons code. So it was kind of a group effort to see how much we could all improve from the one mistake. |Interviewer: And if you were given the option to change one thing about yourself what would it be?|Interviewee: I guess I would probably try to ask more questions. Um  in general if I get directions which are really broad where I don't know where to start searching um instead of asking my higher up i would probably just automatically start looking at everything and start floating around. I guess it would be good to get more like to the point.|Interviewer: And why do you think you should get the job|Interviewee: Um like I mentioned before i think I have a good combination of leadership skills as well as um team player abilities. In addition to this I am pretty qualified I believe in terms of my academic classes that I have taken and also my extracurriculars have demonstrated I have a deep passion for math. And aside from that  I have many internships which have honed my skills in the work place.\n",
      "Ground Truth Scores: 5.41596625181 5.42580511229 5.5121846745\n"
     ]
    }
   ],
   "source": [
    "sample_key = list(val_data.keys())[1] \n",
    "sample = val_data[sample_key]\n",
    "\n",
    "print(\"Sample Key:\", sample_key)\n",
    "print(\"Sample Dialogue:\", sample[\"Transcript\"])\n",
    "print(\"Ground Truth Scores:\", sample.get(\"Overall\"), sample.get(\"RecommendHiring\"), sample.get(\"StructuredAnswers\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenize_dialogue(sample[\"Transcript\"], max_turns=20, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/938010/ipykernel_1046282/584703163.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Scores: {'Overall Score': 4.570374488830566, 'Recommendation Score': 4.2505927085876465, 'Structured Answers Score': 4.107512950897217}\n"
     ]
    }
   ],
   "source": [
    "model = load_trained_model(\"checkpoint1.pth\", device=device)\n",
    "predicted_scores = predict_scores(model, tokenized_input, device=device)\n",
    "formatted_scores = format_scores(predicted_scores)\n",
    "\n",
    "print(\"Predicted Scores:\", formatted_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "Original Dialogue for Candidate 1:\n",
      "Turn 1: so how you doing?\n",
      "Turn 2: great how about you?\n",
      "Turn 3: i'm okay.\n",
      "Turn 4: i'm a little [??? ] by the resurgence of the hurricane but..\n",
      "Turn 5: i understand i got to get home later too and i'm worried.\n",
      "Turn 6: yeah we were half way here and it just turned on a switch lightening winds and everything.\n",
      "Turn 7: oh wow.\n",
      "Turn 8: so yeah.\n",
      "Turn 9: so uh tell me about yourself.\n",
      "Turn 10: okay um i'm a junior here at mit um i'm studying aerospace engineering um my my interests are generally in food mechanics control um design um mostly aircraft but protozoan [??? ] problems in general. um i like to um i like to play music listen to music um i run do some other team sports i like to play hockey.\n",
      "Turn 11: tell me about a time when you demonstrated leadership.\n",
      "Turn 12: um okay so uh one of the projects that i've worked on since coming to mit um was during my freshman year i worked in the in the uh artificial intelligence group in uh cesil [ sp ] which is a lab a computer science lab here at mit.\n",
      "Turn 13: okay.\n",
      "Turn 14: and um i was working on a project with another student actually another freshman uh basically we were tasked with the sort of bringing this project starting this project bring it up to the level where it could be presented at conference.\n",
      "Turn 15: mm - hmm.\n",
      "Turn 16: um together and we were kind of left to work out the cooperation on our own and um there was um as the conference got closer about halfway um about halfway through the year freshman year ah there were a bunch of issues that we had with um with um basically meeting the expectations that had been set before us in terms of what we were going to present. um and finishing in a timely basis.\n",
      "Turn 17: mm - hmm.\n",
      "Turn 18: and it came down to whether we were really going to be able to and there were other basically parts other people's work in the same group where work depended on us finishing in time so it really um really we had to make a decision weather we were just going to basically get done what we could but to the quality standard that the standard of quality that was expected of us or to um try to finish everything and to have other people's work potentially suffer because of our [?? ] hadn't really been up to snuff. and um and so we kind of had a little bit of a disagreement about this\n",
      "Turn 19: tell me about a time when you were working on a team and you were faced with a challenge. how did you solve the problem?\n",
      "Turn 20: okay um so another another thing that i spend a lot of my time doing is called is an organization called design build fly here um it's it's a design competition um run by the iaa which is an aerospace professional organization. and um and uh you can enter a team from any university or group basically anywhere in the world. it's mostly us university though.\n",
      "tensor([4.7251, 4.4594, 3.5760])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/wyue9/scratch/miniconda3/envs/nl2prot/lib/python3.9/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(tokenizer.cls_token)\n",
    "\n",
    "candidate_1_data = test_dataset[1]\n",
    "dialogue_turns, labels = candidate_1_data\n",
    "\n",
    "print(\"Original Dialogue for Candidate 1:\")\n",
    "for turn_idx, input_ids in enumerate(dialogue_turns[\"input_ids\"]):\n",
    "    decoded_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    print(f\"Turn {turn_idx + 1}: {decoded_text}\")\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = \"\"\"\n",
    "User: Hi I'm Bob\n",
    "Interviewer: Hello Bob, nice to meet you. I'm Burdell, the interviewer today. It's great to have you here. Can you start by telling me a little bit about yourself? What motivated you to apply for this role, and what do you know about Microsoft and our products?\n",
    "User: Sure I'm a third year CS major\n",
    "Interviewer: So you're a third-year computer science major. That's impressive. What specific areas of computer science have you been interested in, and do you have any experience with any of the technologies we use here at Microsoft?\n",
    "User: I'm interested in ML. Do you want to start with a technical question\n",
    "Interviewer: Machine Learning is a fascinating field. Let's dive into that. Can you explain the concept of a \"black box\" in machine learning, and how it relates to the problem of feature engineering in a certain type of machine learning model?\n",
    "User: I don't know that. Can you tell me the answer\n",
    "Interviewer: No worries, Bob. A black box refers to a machine learning model that is not transparent or explainable in terms of its internal workings or the relationships between the inputs and outputs. This can make it difficult to interpret or understand the model's performance. In a classification problem, a black box model might output a probability distribution over multiple classes, without providing any insight into which class the model is most likely to predict. In the context of feature engineering, a black box model might not provide any information about the features used in the model, making it hard to understand how the model is selecting those features.\n",
    "Now, let's move on to the next question. Can you explain the concept of a \"k-fold cross-validation\" in machine learning, and how it is used to evaluate the performance of a model?\n",
    "User: k fold is folding pancakes together so that we can check the quality of pancakes\n",
    "Interviewer: (laughs) Ah, nice one, Bob! I think we can safely say that's not a common use of k-fold cross-validation in the culinary world. Seriously though, k-fold cross-validation is a technique used to evaluate the performance of a machine learning model by splitting the data into multiple folds and using each fold for training and the remaining folds for testing. This helps to reduce overfitting and provides a more accurate estimate of the model's performance.\n",
    "Now, let's move on to a technical question. Can you explain the concept of \"stateless\" and \"stateful\" in the context of a neural network, and how they relate to the training process?\n",
    "User: k fold is folding pancakes together so that we can check the quality of pancakes\n",
    "Interviewer: (laughs) Ah, I think we've had enough culinary analogies for one day, Bob. I'm afraid k-fold cross-validation isn't related to pancakes. Seriously though, stateless and stateful are key concepts in neural networks. A stateless network is one where the model's internal state is not preserved between different forward passes, whereas a stateful network is one where the model's internal state is preserved between different forward passes, allowing the model to learn long-term dependencies. In practice, this means that a stateless network might not retain any information about previous inputs, while a stateful network might retain some information and use it to make predictions.\n",
    "Now, let's move on to another question. Can you walk me through an example of how you would implement a simple linear regression model in C, and include error handling and validation to ensure the model is robust and reliable?\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = \"\"\"Turn 1: so how you doing?\n",
    "Turn 2: great how about you?\n",
    "Turn 3: i'm okay.\n",
    "Turn 4: i'm a little [??? ] by the resurgence of the hurricane but..\n",
    "Turn 5: i understand i got to get home later too and i'm worried.\n",
    "Turn 6: yeah we were half way here and it just turned on a switch lightening winds and everything.\n",
    "Turn 7: oh wow.\n",
    "Turn 8: so yeah.\n",
    "Turn 9: so uh tell me about yourself.\n",
    "Turn 10: okay um i'm a junior here at mit um i'm studying aerospace engineering um my my interests are generally in food mechanics control um design um mostly aircraft but protozoan [??? ] problems in general. um i like to um i like to play music listen to music um i run do some other team sports i like to play hockey.\n",
    "Turn 11: tell me about a time when you demonstrated leadership.\n",
    "Turn 12: um okay so uh one of the projects that i've worked on since coming to mit um was during my freshman year i worked in the in the uh artificial intelligence group in uh cesil [ sp ] which is a lab a computer science lab here at mit.\n",
    "Turn 13: okay.\n",
    "Turn 14: and um i was working on a project with another student actually another freshman uh basically we were tasked with the sort of bringing this project starting this project bring it up to the level where it could be presented at conference.\n",
    "Turn 15: mm - hmm.\n",
    "Turn 16: um together and we were kind of left to work out the cooperation on our own and um there was um as the conference got closer about halfway um about halfway through the year freshman year ah there were a bunch of issues that we had with um with um basically meeting the expectations that had been set before us in terms of what we were going to present. um and finishing in a timely basis.\n",
    "Turn 17: mm - hmm.\n",
    "Turn 18: and it came down to whether we were really going to be able to and there were other basically parts other people's work in the same group where work depended on us finishing in time so it really um really we had to make a decision weather we were just going to basically get done what we could but to the quality standard that the standard of quality that was expected of us or to um try to finish everything and to have other people's work potentially suffer because of our [?? ] hadn't really been up to snuff. and um and so we kind of had a little bit of a disagreement about this\n",
    "Turn 19: tell me about a time when you were working on a team and you were faced with a challenge. how did you solve the problem?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Scores for the New Dialogue:\n",
      "Overall Score: 5.14\n",
      "Recommendation Score: 5.12\n",
      "Structured Answers Score: 5.14\n"
     ]
    }
   ],
   "source": [
    "new_dialogue = [\n",
    "    \"Turn 1: so how you doing?\"\n",
    "    \"Turn 2: I hate you\"\n",
    "    \"Turn 3: i'm okay.\"\n",
    "    \"Turn 4: .\"\n",
    "    \"Turn 5: i understand i got to get home later too and i'm worried.\"\n",
    "    \"Turn 6: yeah we were half way here and it just turned on a switch lightening winds and everything.\",\n",
    "]\n",
    "\n",
    "tokenized_input = tokenize_dialogue(new_dialogue)\n",
    "predicted_scores = predict_scores(model, tokenized_input, device=device)\n",
    "formatted_scores = format_scores(predicted_scores)\n",
    "print(\"Predicted Scores for the New Dialogue:\")\n",
    "for label, score in formatted_scores.items():\n",
    "    print(f\"{label}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/wyue9/scratch/miniconda3/envs/nl2prot/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.2.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/hice1/wyue9/scratch/miniconda3/envs/nl2prot/lib/python3.9/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.2.2 when using version 1.4.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "node array from the pickle has an incompatible dtype:\n- expected: {'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity', 'n_node_samples', 'weighted_n_node_samples', 'missing_go_to_left'], 'formats': ['<i8', '<i8', '<i8', '<f8', '<f8', '<i8', '<f8', 'u1'], 'offsets': [0, 8, 16, 24, 32, 40, 48, 56], 'itemsize': 64}\n- got     : [('left_child', '<i8'), ('right_child', '<i8'), ('feature', '<i8'), ('threshold', '<f8'), ('impurity', '<f8'), ('n_node_samples', '<i8'), ('weighted_n_node_samples', '<f8')]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mdata/y_overall_model.joblib\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(\u001b[39m\"\u001b[39m\u001b[39mHow, are you, I am good.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/scratch/miniconda3/envs/nl2prot/lib/python3.9/site-packages/joblib/numpy_pickle.py:658\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m                 \u001b[39mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m--> 658\u001b[0m             obj \u001b[39m=\u001b[39m _unpickle(fobj, filename, mmap_mode)\n\u001b[1;32m    659\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/scratch/miniconda3/envs/nl2prot/lib/python3.9/site-packages/joblib/numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     obj \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    578\u001b[0m     \u001b[39mif\u001b[39;00m unpickler\u001b[39m.\u001b[39mcompat_mode:\n\u001b[1;32m    579\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has been generated with a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mjoblib version less than 0.10. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mPlease regenerate this pickle file.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m                       \u001b[39m%\u001b[39m filename,\n\u001b[1;32m    583\u001b[0m                       \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m~/scratch/miniconda3/envs/nl2prot/lib/python3.9/pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1212\u001b[0m         dispatch[key[\u001b[39m0\u001b[39;49m]](\u001b[39mself\u001b[39;49m)\n\u001b[1;32m   1213\u001b[0m \u001b[39mexcept\u001b[39;00m _Stop \u001b[39mas\u001b[39;00m stopinst:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mreturn\u001b[39;00m stopinst\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/scratch/miniconda3/envs/nl2prot/lib/python3.9/site-packages/joblib/numpy_pickle.py:402\u001b[0m, in \u001b[0;36mNumpyUnpickler.load_build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_build\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    395\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Called to set the state of a newly created object.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \n\u001b[1;32m    397\u001b[0m \u001b[39m    We capture it to replace our place-holder objects, NDArrayWrapper or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39m    NDArrayWrapper is used for backward compatibility with joblib <= 0.9.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     Unpickler\u001b[39m.\u001b[39;49mload_build(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    404\u001b[0m     \u001b[39m# For backward compatibility, we support NDArrayWrapper objects.\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], (NDArrayWrapper, NumpyArrayWrapper)):\n",
      "File \u001b[0;32m~/scratch/miniconda3/envs/nl2prot/lib/python3.9/pickle.py:1717\u001b[0m, in \u001b[0;36m_Unpickler.load_build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1715\u001b[0m setstate \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(inst, \u001b[39m\"\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1716\u001b[0m \u001b[39mif\u001b[39;00m setstate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1717\u001b[0m     setstate(state)\n\u001b[1;32m   1718\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m slotstate \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32msklearn/tree/_tree.pyx:865\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree.__setstate__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msklearn/tree/_tree.pyx:1571\u001b[0m, in \u001b[0;36msklearn.tree._tree._check_node_ndarray\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: node array from the pickle has an incompatible dtype:\n- expected: {'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity', 'n_node_samples', 'weighted_n_node_samples', 'missing_go_to_left'], 'formats': ['<i8', '<i8', '<i8', '<f8', '<f8', '<i8', '<f8', 'u1'], 'offsets': [0, 8, 16, 24, 32, 40, 48, 56], 'itemsize': 64}\n- got     : [('left_child', '<i8'), ('right_child', '<i8'), ('feature', '<i8'), ('threshold', '<f8'), ('impurity', '<f8'), ('n_node_samples', '<i8'), ('weighted_n_node_samples', '<f8')]"
     ]
    }
   ],
   "source": [
    "model = joblib.load(\"data/y_overall_model.joblib\")\n",
    "prediction = model.predict(\"How, are you, I am good.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desp2protein",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
